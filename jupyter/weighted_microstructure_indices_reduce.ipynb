{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import pandas as pd\n",
    "\n",
    "import dipy.tracking.utils\n",
    "import dipy.tracking.streamline\n",
    "\n",
    "import os, sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_ids = [1, 4, 6, 7, 10, 11, 12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/user/pfilipia/home/inria/chu_nice_inria/patients_dmri\"\n",
    "\n",
    "def get_patient_dir(patient_id):\n",
    "    return os.path.join(\n",
    "        data_dir, \"patient%02d/bids/sub-patient%02d/ses-presurgical\" % (patient_id, patient_id)\n",
    "    )\n",
    "\n",
    "def get_connectivity_dir(patient_id):\n",
    "    return os.path.join(\n",
    "        get_patient_dir(patient_id), \"connectivity\"\n",
    "    )\n",
    "\n",
    "def get_connectivity_file(patient_id):\n",
    "    return os.path.join(\n",
    "        get_connectivity_dir(patient_id), \"connections_common_avg_seed5k_after_shift.csv\"\n",
    "    )    \n",
    "\n",
    "def get_percentile_connectivity_file(patient_id, percentile):\n",
    "    return os.path.join(\n",
    "        get_connectivity_dir(patient_id), \"connections_common_avg_seed5k_after_shift_p%02d.csv\" % percentile\n",
    "    )    \n",
    "\n",
    "def get_tck_streamlines_dir(patient_id):\n",
    "    return os.path.join(\n",
    "        get_connectivity_dir(patient_id), \"tck_streamlines\"\n",
    "    )\n",
    "\n",
    "def get_tck_streamlines_file(patient_id, stimulation_site, recording_electrode):\n",
    "    return os.path.join(\n",
    "        get_tck_streamlines_dir(patient_id),\n",
    "        \"%s_%s_diam10_in00.tck\" % (stimulation_site, recording_electrode)\n",
    "    )\n",
    "\n",
    "def get_chunk_file(patient_id, stimulation_site, recording_electrode, chunk_id):\n",
    "    return os.path.join(\n",
    "        get_tck_streamlines_dir(patient_id),\n",
    "        \"%s_%s_indices_%d.csv\" % (stimulation_site, recording_electrode, chunk_id)\n",
    "    )\n",
    "\n",
    "def get_weights_file(patient_id, stimulation_site, recording_electrode):\n",
    "    return os.path.join(\n",
    "        get_tck_streamlines_dir(patient_id),\n",
    "        \"%s_%s_diam10_in00_weight.txt\" % (stimulation_site, recording_electrode)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def reduce_indices(patient_id, limit_length_percentile = 100):\n",
    "\n",
    "    all_indices = [\n",
    "        'fa', 'md', 'ad', 'rd',\n",
    "        'rtop', 'rtap', 'rtpp', 'msd', 'qiv_e9', 'ng', 'ng_perp', 'ng_par'\n",
    "    ]\n",
    "\n",
    "    output_cols = [\n",
    "        'fa', 'fa_std', 'md', 'md_std', 'ad', 'ad_std', 'rd', 'rd_std',\n",
    "        'rtop', 'rtap', 'rtpp', 'msd', 'qiv_e9', 'ng', 'ng_perp', 'ng_par'\n",
    "    ]\n",
    "    \n",
    "    connectivity_file_cols = {\n",
    "        'fa': 'mean_fa', 'fa_std': 'std_fa', 'md': 'mean_md', 'md_std': 'std_md',\n",
    "        'ad': 'mean_ad', 'ad_std': 'std_ad', 'rd': 'mean_rd', 'rd_std': 'std_rd',\n",
    "        'rtop': 'mean_rtop', 'rtap': 'mean_rtap', 'rtpp': 'mean_rtpp', 'msd': 'mean_msd',\n",
    "        'qiv_e9': 'mean_qiv', 'ng': 'ng', 'ng_perp': 'ng_perp', 'ng_par': 'ng_par'\n",
    "    }\n",
    "\n",
    "    data_pd = {}\n",
    "\n",
    "    output_lines = [] \n",
    "    data_pd[patient_id] = pd.read_csv(get_connectivity_file(patient_id), skipinitialspace=True)\n",
    "\n",
    "    for data_row in data_pd[patient_id].iterrows():\n",
    "        \n",
    "        data_chunks = []\n",
    "        for chunk_id in range(10):\n",
    "            \n",
    "            try:\n",
    "                \n",
    "                f = open(\n",
    "                    get_chunk_file(\n",
    "                        patient_id, data_row[1]['stimulation_site'], data_row[1]['recording_electrode'], chunk_id\n",
    "                    ), 'r'\n",
    "                )\n",
    "                str_input = f.read().replace(\"\\n\\t\", \"\\t\").replace(\"\\t\\n\", \"\\n\")\n",
    "                f.close()\n",
    "\n",
    "                chunk_pd = pd.read_csv(pd.compat.StringIO(str_input), sep='\\t', skipinitialspace=True)\n",
    "\n",
    "                for idx in all_indices:\n",
    "                    chunk_pd = chunk_pd[chunk_pd[idx] > 0]\n",
    "                \n",
    "                if not 'length' in chunk_pd.columns:\n",
    "                    continue\n",
    "                        \n",
    "                data_chunks.append(chunk_pd[['weight', 'length'] + all_indices])\n",
    "                \n",
    "            except:\n",
    "                print(\"Exception\")\n",
    "                None\n",
    "                \n",
    "        output_line = \"%s %s \" % (data_row[1]['stimulation_site'], data_row[1]['recording_electrode'])\n",
    "\n",
    "        try:\n",
    "            all_chunks_pd = pd.concat(data_chunks)\n",
    "                      \n",
    "            if limit_length_percentile > 100:\n",
    "                min_length_percentile = limit_length_percentile - 100\n",
    "            else:\n",
    "                min_length_percentile = 0\n",
    "\n",
    "            percentile_value = np.percentile(all_chunks_pd['length'].values, np.minimum(100, limit_length_percentile))\n",
    "            percentile_pd = all_chunks_pd[all_chunks_pd['length'] <= percentile_value]\n",
    "\n",
    "            min_length_percentile_value = np.percentile(all_chunks_pd['length'].values, min_length_percentile)\n",
    "            percentile_pd = percentile_pd[percentile_pd['length'] >= min_length_percentile_value]\n",
    "            \n",
    "            for output_col in output_cols:\n",
    "                if output_col in percentile_pd.keys():\n",
    "                    output_line += \"%f \" % np.average(\n",
    "                        percentile_pd[[output_col]].values, weights=percentile_pd[['weight']].values\n",
    "                    )\n",
    "                else:\n",
    "                    output_line += \"0 \"\n",
    "        except:\n",
    "            for output_col in output_cols:\n",
    "                output_line += \"0 \"\n",
    "                    \n",
    "        output_lines.append(output_line.strip())\n",
    "\n",
    "    indices = []\n",
    "    for output_line in sorted(output_lines):\n",
    "        indices_row = np.array(output_line[8:].split(\" \"), dtype=float)\n",
    "        indices.append(indices_row)\n",
    "        \n",
    "    data_pd[patient_id][[connectivity_file_cols[col] for col in output_cols]] = np.array(indices)\n",
    "    data_pd[patient_id].to_csv(\n",
    "        get_percentile_connectivity_file(patient_id, limit_length_percentile), sep=',', index=False\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PATIENT_ID = 1, PERCENTILE = 100\n",
      "PATIENT_ID = 1, PERCENTILE = 110\n",
      "PATIENT_ID = 1, PERCENTILE = 120\n",
      "PATIENT_ID = 1, PERCENTILE = 130\n",
      "PATIENT_ID = 1, PERCENTILE = 140\n",
      "PATIENT_ID = 1, PERCENTILE = 150\n",
      "PATIENT_ID = 1, PERCENTILE = 160\n",
      "PATIENT_ID = 1, PERCENTILE = 170\n",
      "PATIENT_ID = 1, PERCENTILE = 180\n",
      "PATIENT_ID = 1, PERCENTILE = 190\n",
      "PATIENT_ID = 4, PERCENTILE = 100\n",
      "PATIENT_ID = 4, PERCENTILE = 110\n",
      "PATIENT_ID = 4, PERCENTILE = 120\n",
      "PATIENT_ID = 4, PERCENTILE = 130\n",
      "PATIENT_ID = 4, PERCENTILE = 140\n",
      "PATIENT_ID = 4, PERCENTILE = 150\n",
      "PATIENT_ID = 4, PERCENTILE = 160\n",
      "PATIENT_ID = 4, PERCENTILE = 170\n",
      "PATIENT_ID = 4, PERCENTILE = 180\n",
      "PATIENT_ID = 4, PERCENTILE = 190\n",
      "PATIENT_ID = 6, PERCENTILE = 100\n",
      "PATIENT_ID = 6, PERCENTILE = 110\n",
      "PATIENT_ID = 6, PERCENTILE = 120\n",
      "PATIENT_ID = 6, PERCENTILE = 130\n",
      "PATIENT_ID = 6, PERCENTILE = 140\n",
      "PATIENT_ID = 6, PERCENTILE = 150\n",
      "PATIENT_ID = 6, PERCENTILE = 160\n",
      "PATIENT_ID = 6, PERCENTILE = 170\n",
      "PATIENT_ID = 6, PERCENTILE = 180\n",
      "PATIENT_ID = 6, PERCENTILE = 190\n",
      "PATIENT_ID = 7, PERCENTILE = 100\n",
      "PATIENT_ID = 7, PERCENTILE = 110\n",
      "PATIENT_ID = 7, PERCENTILE = 120\n",
      "PATIENT_ID = 7, PERCENTILE = 130\n",
      "PATIENT_ID = 7, PERCENTILE = 140\n",
      "PATIENT_ID = 7, PERCENTILE = 150\n",
      "PATIENT_ID = 7, PERCENTILE = 160\n",
      "PATIENT_ID = 7, PERCENTILE = 170\n",
      "PATIENT_ID = 7, PERCENTILE = 180\n",
      "PATIENT_ID = 7, PERCENTILE = 190\n",
      "PATIENT_ID = 10, PERCENTILE = 100\n",
      "PATIENT_ID = 10, PERCENTILE = 110\n",
      "PATIENT_ID = 10, PERCENTILE = 120\n",
      "PATIENT_ID = 10, PERCENTILE = 130\n",
      "PATIENT_ID = 10, PERCENTILE = 140\n",
      "PATIENT_ID = 10, PERCENTILE = 150\n",
      "PATIENT_ID = 10, PERCENTILE = 160\n",
      "PATIENT_ID = 10, PERCENTILE = 170\n",
      "PATIENT_ID = 10, PERCENTILE = 180\n",
      "PATIENT_ID = 10, PERCENTILE = 190\n",
      "PATIENT_ID = 11, PERCENTILE = 100\n",
      "PATIENT_ID = 11, PERCENTILE = 110\n",
      "PATIENT_ID = 11, PERCENTILE = 120\n",
      "PATIENT_ID = 11, PERCENTILE = 130\n",
      "PATIENT_ID = 11, PERCENTILE = 140\n",
      "PATIENT_ID = 11, PERCENTILE = 150\n",
      "PATIENT_ID = 11, PERCENTILE = 160\n",
      "PATIENT_ID = 11, PERCENTILE = 170\n",
      "PATIENT_ID = 11, PERCENTILE = 180\n",
      "PATIENT_ID = 11, PERCENTILE = 190\n",
      "PATIENT_ID = 12, PERCENTILE = 100\n",
      "PATIENT_ID = 12, PERCENTILE = 110\n",
      "PATIENT_ID = 12, PERCENTILE = 120\n",
      "PATIENT_ID = 12, PERCENTILE = 130\n",
      "PATIENT_ID = 12, PERCENTILE = 140\n",
      "PATIENT_ID = 12, PERCENTILE = 150\n",
      "PATIENT_ID = 12, PERCENTILE = 160\n",
      "PATIENT_ID = 12, PERCENTILE = 170\n",
      "PATIENT_ID = 12, PERCENTILE = 180\n",
      "PATIENT_ID = 12, PERCENTILE = 190\n"
     ]
    }
   ],
   "source": [
    "for patient_id in patient_ids:\n",
    "    for percentile in [100, 110, 120, 130, 140, 150, 160, 170, 180, 190]: # [1, 2, 5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]:\n",
    "        print(\"PATIENT_ID = %d, PERCENTILE = %d\" % (patient_id, percentile))\n",
    "        reduce_indices(patient_id, percentile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
